{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Week7-Checkpointing-TransferLearning.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"mamiXm5pltDZ","colab_type":"text"},"source":["# Checkpointing and Transfer Learning\n","\n","### Welcome to the 7th Lab of 42028: Deep Learning and CNN!\n","\n","In this  Lab/Tutorial session you will be learning how to checkpoint a model and also restart training from an existing checkpoint. In the second session you will implement Transfer learning/fine-tuning.\n","\n","Designing of Alexnet architecture will also be discussed.\n","\n","So lets get started!\n","\n","## Tutorial:\n","1. Checkpointing or saving trained model\n","2. Transfer learning\n","3. Classic CNNs\n","\n","## Tasks for this week:\n","\n","1. Implementation of CNN for Dogs and Cats classification using Keras API. \n","2. Save the snapshot of trained model using checkpoint\n","3. Loading the weights of trained model and start training again.\n","3. Using out-of-the-box models for classification\n","4. Transfer learning/fine-tuning from already trained model\n","5. Implement Alexnet.\n"]},{"cell_type":"markdown","metadata":{"id":"4IG-E4uMo8al","colab_type":"text"},"source":["##Task-1: Implementing CNN for Dogs and Cats classification using Keras API"]},{"cell_type":"markdown","metadata":{"id":"CrgipiyWnlOF","colab_type":"text"},"source":["### Step 1: Import required packages\n","\n","we will need tensorflow, numpy, os and keras\n"]},{"cell_type":"code","metadata":{"id":"cZzYq6qGXOUw","colab_type":"code","colab":{}},"source":["import os\n","import tensorflow as tf\n","import zipfile\n","import matplotlib.pyplot as plt\n","from tensorflow.keras import layers\n","from tensorflow.keras import Model\n","from sklearn.preprocessing import LabelBinarizer\n","from keras.callbacks import ModelCheckpoint\n","from keras.optimizers import SGD\n","from keras.datasets import cifar10\n","from tensorflow.keras.optimizers import RMSprop\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hgqx7vZnw4_W","colab_type":"text"},"source":["### Step 2: Download the Cats & Dogs dataset"]},{"cell_type":"code","metadata":{"id":"Gz2sswLyw9hZ","colab_type":"code","colab":{}},"source":["!wget --no-check-certificate \\\n","    https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip \\\n","    -O /tmp/cats_and_dogs_filtered.zip"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yXwYO33qxB2Y","colab_type":"code","colab":{}},"source":["local_zip = '/tmp/cats_and_dogs_filtered.zip'\n","zip_ref = zipfile.ZipFile(local_zip, 'r')\n","zip_ref.extractall('/tmp')\n","zip_ref.close()\n","\n","base_dir = '/tmp/cats_and_dogs_filtered'\n","train_dir = os.path.join(base_dir, 'train')\n","validation_dir = os.path.join(base_dir, 'validation')\n","\n","# Directory with our training cat pictures\n","train_cats_dir = os.path.join(train_dir, 'cats')\n","\n","# Directory with our training dog pictures\n","train_dogs_dir = os.path.join(train_dir, 'dogs')\n","\n","# Directory with our validation cat pictures\n","validation_cats_dir = os.path.join(validation_dir, 'cats')\n","\n","# Directory with our validation dog pictures\n","validation_dogs_dir = os.path.join(validation_dir, 'dogs')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5xTGEynaxUfL","colab_type":"text"},"source":["### Step 3:  Design the CNN Architecture \n","\n","Design the following CNN architecture:\n","\n","\n","<img src='http://drive.google.com/uc?export=view&id=1EAWFwp7T92q3Lm1ZrX9A2-wnvhfAfzSF' alt='Conv'>\n","\n","Input: $150 X 150 X 3$ image\n","\n","No. of filters:\n","- Conv1 : 32, 3x3\n","- Conv2 : 64, 3x3\n","- Conv4 : 128, 3x3\n","- Conv4 : 128, 3x3\n","\n","Activation function in CONV layer: Relu\n","\n","Pool: MaxPooling, 2x2\n","\n","FC Layer: 512 nodes, Activation : ReLu\n","\n","Activation function in Output layer : sigmoid, \n","\n","**Hint:** Use Conv2D(), MaxPooling2D(), Flatten(), and Dense()"]},{"cell_type":"code","metadata":{"id":"OerFd75RoIWz","colab_type":"code","colab":{}},"source":["model = tf.keras.models.Sequential([\n","    \n","    ## Start Your Code Here ###\n","    \n","    \n","])\n","## End Your Code Here ###"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uBOCW--vh6Wn","colab_type":"text"},"source":["### Step 4. Model Compilation"]},{"cell_type":"code","metadata":{"id":"yBr8HgvoyMUc","colab_type":"code","colab":{}},"source":["## Start Your Code Here ##\n","## Hint: Loss is 'binary_crossentropy',\n","## optimizer: RMSprop(lr=1e-4),\n","## metrics: 'acc'\n","model.compile()## Complete the missing paramenters\n","## End Your Code Here ###"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GGZqihrPiBjs","colab_type":"text"},"source":["### Step 5. Using Image generator to load images and generate labels automatically. Image generator also resizes the images."]},{"cell_type":"code","metadata":{"id":"jj7_kqv6yR7D","colab_type":"code","colab":{}},"source":["# All images will be rescaled by 1./255\n","## Start Your Code Here ##\n","\n","train_datagen = \n","test_datagen = \n","\n","# Flow training images in batches of 20 using train_datagen generator\n","train_generator = train_datagen.flow_from_directory(\n","        train_dir,  # This is the source directory for training images\n","        target_size=(),  #Complete the paramenters, All images will be resized to 150x150,\n","        batch_size=20,\n","        # Since we use binary_crossentropy loss, we need binary labels\n","        class_mode='binary')\n","\n","# Flow validation images in batches of 20 using test_datagen generator\n","validation_generator = test_datagen.flow_from_directory(\n","        validation_dir,\n","        target_size=()),#Complete the paramenters,\n","        batch_size=20,\n","        class_mode='binary')\n","\n","## End Your Code Here ##"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GTJvAARCiQbm","colab_type":"text"},"source":["##Task-2: Saving the snapshot of model as checkpoint\n","\n"]},{"cell_type":"markdown","metadata":{"id":"-BWSpqW6NYJE","colab_type":"text"},"source":["###Step 6: Checkpointing"]},{"cell_type":"code","metadata":{"id":"fBmvIQqkyqx7","colab_type":"code","colab":{}},"source":["#checkpoint = ModelCheckpoint('weights.{epoch:02d}-{val_loss:.2f}.hdf5', monitor='val_loss', save_best_only=True, verbose=1, period=3)\n","filepath='/tmp/weights.{epoch:02d}-{val_loss:.2f}.hdf5'\n","checkpoint=tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KeXwrBbnibPp","colab_type":"text"},"source":["### Step 7. Training the model"]},{"cell_type":"code","metadata":{"id":"JY2IwBMvyiLq","colab_type":"code","colab":{}},"source":["history = model.fit_generator(\n","      train_generator,\n","      steps_per_epoch=100,  # 2000 images = batch_size * steps\n","      epochs=10,\n","      validation_data=validation_generator,\n","      validation_steps=10,  # 1000 images = batch_size * steps\n","      callbacks = [checkpoint],\n","      verbose=2)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gXEg9DqPNkMW","colab_type":"text"},"source":["##Task 3: Loading the weights of trained model and start training again."]},{"cell_type":"markdown","metadata":{"id":"STof3OBrdWUy","colab_type":"text"},"source":["### Step 8.  Retraining from saved model"]},{"cell_type":"code","metadata":{"id":"PZSdysPLcfOA","colab_type":"code","colab":{}},"source":["## Go to the /tmp folder and copy the name of the last saved model\n","\n","## Start Your Code Here ##\n","model_modify=tf.keras.models.load_model() # Complete the code\n","\n","## End Your Code Here ##"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4y32QymeiqDB","colab_type":"text"},"source":["### Step 9. Compile the modified model "]},{"cell_type":"code","metadata":{"id":"XPiolP9mdDpF","colab_type":"code","colab":{}},"source":["## Start Your Code Here ##\n","## Hint: Loss is 'binary_crossentropy',\n","## optimizer: RMSprop(lr=1e-4),\n","## metrics: 'acc'\n","model_modify.compile()## Complete the missing paramenters\n","## End Your Code Here ###\n","\n","model_modify.summary()\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ew5AXJW_dw2f","colab_type":"code","colab":{}},"source":["# All images will be rescaled by 1./255\n","train_datagen = ImageDataGenerator(rescale=1./255)\n","test_datagen = ImageDataGenerator(rescale=1./255)\n","\n","## Start Your Code Here ###\n","\n","# Flow training images in batches of 20 using train_datagen generator\n","train_generator = train_datagen.flow_from_directory(\n","        train_dir,  # This is the source directory for training images\n","        target_size=(), ##Complete the code # All images will be resized to 150x150\n","        batch_size=20,\n","        # Since we use binary_crossentropy loss, we need binary labels\n","        class_mode='binary')\n","\n","# Flow validation images in batches of 20 using test_datagen generator\n","validation_generator = test_datagen.flow_from_directory(\n","        validation_dir,\n","        target_size=(), ## Complete the code\n","        batch_size=20,\n","        class_mode='binary')\n","\n","## End Your Code Here ###"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HU-zYEG3cWYM","colab_type":"code","colab":{}},"source":["## Adding checkpoints after every 2-epochs \n","filepath='/tmp/weights_modified.{epoch:02d}-{val_loss:.2f}.hdf5'\n","checkpoint=tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=2)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fq-vNEFheD0c","colab_type":"code","colab":{}},"source":["# Train the model\n","history = model_modify.fit_generator(\n","      train_generator,\n","      steps_per_epoch=100,  # 2000 images = batch_size * steps\n","      epochs=10,\n","      validation_data=validation_generator,\n","      validation_steps=10,  # 1000 images = batch_size * steps\n","      callbacks = [checkpoint],\n","      verbose=2)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BDguzpGcORcB","colab_type":"text"},"source":["## Task 4: Using out-of-the-box models for classification"]},{"cell_type":"markdown","metadata":{"id":"DEraXTowykNv","colab_type":"text"},"source":["## Task 5: Transfer Learning\n","\n","[How to use pretrained networks for out of the box classification](https://keras.io/applications/)"]},{"cell_type":"markdown","metadata":{"id":"CKRv_dX0jE9y","colab_type":"text"},"source":["### Step 1. Mount the google drive."]},{"cell_type":"code","metadata":{"id":"LHe26itR9fRe","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NBMtCxhJBnIi","colab_type":"code","colab":{}},"source":["cd /content/gdrive/My Drive/42028-DL-CNN-2020/Week7-Lab7/images"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A4VEJWkQjNqM","colab_type":"text"},"source":["### Step 2. Using ResNet50 pretrained model for classification "]},{"cell_type":"code","metadata":{"id":"rkEBPQG65i9J","colab_type":"code","colab":{}},"source":["from keras.applications.resnet50 import ResNet50\n","from keras.preprocessing import image\n","from keras.applications.resnet50 import preprocess_input, decode_predictions\n","import numpy as np\n","\n","# Load the ResNet50 model with pretrained weights\n","model = ResNet50(weights='imagenet')\n","\n","img_path = '/content/gdrive/My Drive/42028-DL-CNN/Week7-Lab7/images/cat.jpg'\n","img = image.load_img(img_path, target_size=(224, 224))\n","plt.imshow(img)\n","x = image.img_to_array(img)\n","x = np.expand_dims(x, axis=0)\n","x = preprocess_input(x)\n","\n","features = model.predict(x)\n","preds = model.predict(x)\n","print('Predicted:', decode_predictions(preds, top=3)[0])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3xIRHR6JniK_","colab_type":"text"},"source":["### Step 3: Transfer learning\n","\n","We will use the VGG16 CNN architecture as the base model and adapt/re-train the FC layers to Dogs and Cats classification task.\n","\n","VGG16 CNN architecture is given below:\n","\n","![alt text](https://neurohive.io/wp-content/uploads/2018/11/vgg16.png)"]},{"cell_type":"code","metadata":{"id":"wgsQ7VAJaKkS","colab_type":"code","colab":{}},"source":["from keras.applications import VGG16\n","conv_base = VGG16(weights='imagenet',include_top=False, input_shape=(150, 150, 3))\n","\n","\n","from keras import models\n","from keras import layers\n","from keras import optimizers\n","\n","# Load the CONV layers of VGG16 model and add the FC layers\n","\n","model = models.Sequential()\n","model.add(conv_base)\n","model.add(layers.Flatten())\n","model.add(layers.Dense(256, activation='relu'))\n","model.add(layers.Dense(1, activation='sigmoid'))\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vb18OL3mSmi4","colab_type":"code","colab":{}},"source":["conv_base.summary()\n","for layer in conv_base.layers[:-4]:\n","    layer.trainable = False\n"," \n","# Check the trainable status of the individual layers\n","for layer in conv_base.layers:\n","    print(layer, layer.trainable)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZO4WP1eIGp92","colab_type":"code","colab":{}},"source":["model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Vz9pRPjjomg5","colab_type":"text"},"source":["### Step 4:  Training CNN with ImageDataGenerator"]},{"cell_type":"code","metadata":{"id":"6NG8OyQ4asBu","colab_type":"code","colab":{}},"source":["# Updated to do image augmentation\n","train_datagen = ImageDataGenerator(\n","      rescale=1./255,\n","      rotation_range=40,\n","      width_shift_range=0.2,\n","      height_shift_range=0.2,\n","      shear_range=0.2,\n","      zoom_range=0.2,\n","      horizontal_flip=True,\n","      fill_mode='nearest')\n","\n","test_datagen = ImageDataGenerator(rescale=1./255)\n","\n","## Start Your Code Here ###\n","\n","# Flow training images in batches of 20 using train_datagen generator\n","train_generator = train_datagen.flow_from_directory(\n","        train_dir,  # This is the source directory for training images\n","        target_size=(),  #Complete the parameters # All images will be resized to 150x150\n","        batch_size=20,\n","        # Since we use binary_crossentropy loss, we need binary labels\n","        class_mode='binary')\n","\n","# Flow validation images in batches of 20 using test_datagen generator\n","validation_generator = test_datagen.flow_from_directory(\n","        validation_dir,\n","        target_size=(), #Complete the parameters\n","        batch_size=20,\n","        class_mode='binary')\n","\n","## Start Your Code Here ###"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JqzAtAY7PI3Z","colab_type":"code","colab":{}},"source":["model.compile(loss='binary_crossentropy',\n","              optimizer=optimizers.RMSprop(lr=1e-4),\n","              metrics=['acc'])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CbSRkelDa13h","colab_type":"code","colab":{}},"source":["# Train the model\n","history = model.fit_generator(\n","      train_generator,\n","      steps_per_epoch=100,  # 2000 images = batch_size * steps\n","      epochs=10,\n","      validation_data=validation_generator,\n","      validation_steps=50,  # 1000 images = batch_size * steps\n","      verbose=2)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o35LXjMapnVB","colab_type":"text"},"source":["### Step 5:  Visualization of results "]},{"cell_type":"code","metadata":{"id":"EpCUC0x9a6-5","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","acc = history.history['acc']\n","val_acc = history.history['val_acc']\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","\n","epochs = range(len(acc))\n","\n","plt.plot(epochs, acc, 'bo', label='Training accuracy')\n","plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n","plt.title('Training and validation accuracy')\n","\n","plt.figure()\n","\n","plt.plot(epochs, loss, 'bo', label='Training Loss')\n","plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n","plt.title('Training and validation loss')\n","plt.legend()\n","\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MYQilm-BgXfq","colab_type":"text"},"source":["## Task 6: AlexNet implementation\n","\n","The Alexnet CNN architecture is show in the diagram given below:\n","\n","![alt text](https://www.oreilly.com/library/view/tensorflow-for-deep/9781491980446/assets/tfdl_0106.png)"]},{"cell_type":"markdown","metadata":{"id":"qQshPRNNIMpx","colab_type":"text"},"source":["### Create the Alexnet architecture"]},{"cell_type":"code","metadata":{"id":"ouh4cIuygZq3","colab_type":"code","colab":{}},"source":["model = tf.keras.models.Sequential([\n","    #Conv_1          #original model was built for input shape of 224X224\n","    tf.keras.layers.Conv2D(96, (11,11),strides=4, padding='valid', activation='relu', input_shape=(224, 224, 3)),\n","    # Pooling_1\n","    tf.keras.layers.MaxPooling2D((2, 2), strides=(2,2),padding='valid'),\n","    # Batch Normalisation_1\n","    tf.keras.layers.BatchNormalization(),\n","    # Conv_2\n","    tf.keras.layers.Conv2D(256, (11,11),strides=1, padding='valid', activation='relu'),\n","    # Pooling_2\n","    tf.keras.layers.MaxPooling2D((2, 2), strides=(2,2),padding='valid'),\n","    #Batch Normalisation_2\n","    tf.keras.layers.BatchNormalization(),\n","    # Conv_3\n","    tf.keras.layers.Conv2D(384, (3,3),strides=1, padding='valid', activation='relu'),\n","    # Batch Normalisation_3\n","    tf.keras.layers.BatchNormalization(),\n","    # Conv_4\n","    tf.keras.layers.Conv2D(384, (3,3),strides=1, padding='valid', activation='relu'),\n","    # Batch Normalisation_3\n","    tf.keras.layers.BatchNormalization(),\n","    #conv_5\n","    tf.keras.layers.Conv2D(256, (3,3),strides=1, padding='valid', activation='relu'),\n","    #pooling_3\n","    tf.keras.layers.MaxPooling2D((2, 2), strides=(2,2),padding='valid'),\n","    #Batch Normalization_4\n","    tf.keras.layers.BatchNormalization(),\n","    tf.keras.layers.Flatten(),\n","    #Dense layer_1\n","    tf.keras.layers.Dense(4096, activation='relu'),\n","    tf.keras.layers.Dropout(0.5),\n","    tf.keras.layers.BatchNormalization(),\n","    #Dense layer_2\n","    tf.keras.layers.Dense(4096, activation='relu'),\n","    tf.keras.layers.Dropout(0.5),\n","    tf.keras.layers.BatchNormalization(),\n","    #Dense layer_3\n","    tf.keras.layers.Dense(1000, activation='relu'),\n","    tf.keras.layers.Dropout(0.5),\n","    tf.keras.layers.BatchNormalization(),\n","    tf.keras.layers.Dense(1, activation='sigmoid')\n","    ])\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hVBYywzNHHDp","colab_type":"text"},"source":["###Create the dataset by resizing the images to 224X224 for training Alexnet."]},{"cell_type":"code","metadata":{"id":"lScGc7gS3CtP","colab_type":"code","colab":{}},"source":["# Updated to do image augmentation\n","train_datagen = ImageDataGenerator(\n","      rescale=1./255,\n","      rotation_range=40,\n","      width_shift_range=0.2,\n","      height_shift_range=0.2,\n","      shear_range=0.2,\n","      zoom_range=0.2,\n","      horizontal_flip=True,\n","      fill_mode='nearest')\n","\n","test_datagen = ImageDataGenerator(rescale=1./255)\n","\n","## Start Your Code Here ###\n","\n","# Flow training images in batches of 20 using train_datagen generator\n","train_generator = train_datagen.flow_from_directory(\n","        train_dir,  # This is the source directory for training images\n","        target_size=(),  # Complete the code, All images will be resized to 224x224\n","        batch_size=20,\n","        # Since we use binary_crossentropy loss, we need binary labels\n","        class_mode='binary')\n","\n","# Flow validation images in batches of 20 using test_datagen generator\n","validation_generator = test_datagen.flow_from_directory(\n","        validation_dir,\n","        target_size=(), # Complete the code, All images will be resized to 224x224\n","        batch_size=20,\n","        class_mode='binary')\n","\n","## End Your Code Here ###"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"z2MPgX2n3xzV","colab_type":"code","colab":{}},"source":["model.compile(loss='binary_crossentropy',\n","              optimizer=RMSprop(lr=1e-4),\n","              metrics=['acc'])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eOuv5E3Y36UM","colab_type":"code","colab":{}},"source":["# Train the model\n","history = model.fit_generator(\n","      train_generator,\n","      steps_per_epoch=100,  # 2000 images = batch_size * steps\n","      epochs=20,\n","      validation_data=validation_generator,\n","      validation_steps=10,  # 1000 images = batch_size * steps\n","      #callbacks = [checkpoint],\n","      verbose=2)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TL1AfuEwHq2C","colab_type":"text"},"source":["###Visualization of results\n","\n","\n","This is just for illustration only. The actual accuracy may vary based on the number of epochs"]},{"cell_type":"code","metadata":{"id":"ZM0Hgj1D4LTt","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","acc = history.history['acc']\n","val_acc = history.history['val_acc']\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","\n","epochs = range(len(acc))\n","\n","plt.plot(epochs, acc, 'bo', label='Training accuracy')\n","plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n","plt.title('Training and validation accuracy')\n","\n","plt.figure()\n","\n","plt.plot(epochs, loss, 'bo', label='Training Loss')\n","plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n","plt.title('Training and validation loss')\n","plt.legend()\n","\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-Maem5Yv_FCn","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}