{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Week5-LogisticRegression-NeuralNetwork-Part-2.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"WAAlSUKe06dA","colab_type":"text"},"source":["# Logistic Regression using Neural Network: Part -2\n","\n","Welcome to the 4th Lab of 42028: Deep Learning and CNN!\n","\n","In this week you will be implementing a Logistic Regression classifier using Neural Network. Specifically, you will implement the activation function, cost/loss function and gradient descent algorithm. \n","\n","So lets get started!"]},{"cell_type":"markdown","metadata":{"id":"_kF22oPX-IWa","colab_type":"text"},"source":["## Binary Classification (Logistic Regression) Overview\n","\n","\n","<img src='http://drive.google.com/uc?export=view&id=1LFtuvxhDn1rbZjeuzw2uAXI6Y9I5qyaa' alt='Conv'>\n","\n","\n","\n","Activation function $a = \\sigma( w^T x + b) = \\frac{1}{1 + e^{-(w^T x + b)}}$\n","\n","Loss function $L(a, y) = -y\\log a+(1-y)\\log(1-a)$ \n","\n","\n","\n","<img src='http://drive.google.com/uc?export=view&id=1TrjdisDOCqqj0uaAC9zkPGpEAXBEtRA2' alt='Conv'>\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"BcuTioANFvDd","colab_type":"text"},"source":["## Tasks: \n","\n","1. Complete the implementation of Sigmoid activation function \n","2. Complete the implementation of the Log loss cost function\n","3. Complete the implementation of Gradient Descent algorithm\n","4. Train and test the logistic classifier to classify cats and dogs\n"]},{"cell_type":"code","metadata":{"id":"NjqkE5VlxuJW","colab_type":"code","colab":{}},"source":["## Import the required packages ##\n","import math, numpy as np\n","import sklearn.datasets\n","import matplotlib.pyplot as plt\n","\n","import os\n","import h5py\n","import glob\n","import cv2\n","from keras.preprocessing import image"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZC-aJw3MGySa","colab_type":"text"},"source":["## Task 1: Complete the implementation of Sigmoid activation function \n","\n","A sigmoid function is of a form:\n","\n","$\\sigma( w^T x + b) = \\frac{1}{1 + e^{-(w^T x + b)}}$\n","\n","Let  $z=w^T x + b,$\n","\n","Hence, $\\sigma(z) = \\frac{1}{1 + e^{-(z)}}$\n","\n","\n","Write a function below to calculate $\\sigma(z)$ is Python.\n","\n","Hint:\n","\n","**exp(x)** --> Return e**x. \n","\n","\n","**Reference**: https://docs.python.org/2/library/math.html"]},{"cell_type":"code","metadata":{"id":"8apsHlXkS7nX","colab_type":"code","colab":{}},"source":["def sigmoid(z):\n","    \"\"\"\n","    Returns the sigmoid of z\n","\n","    Arguments:\n","    z --> A scalar or numpy array of any size.\n","\n","    Return:\n","    s --> sigmoid(z)\n","    \"\"\"\n","\n","    ### WRITE YOUR CODE HERE ### (~ 1 line of code)\n","    \n","    s = \n","    \n","    ### END YOUR CODE HERE ###\n","    \n","    return s"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i3vQacHuKE_b","colab_type":"text"},"source":["## Task-2: Implement the negative log-likelihood loss (log loss)\n","\n","$L(a, y) =\\frac{1}{m} \\sum_{i=1}^m - y\\log a+(1-y)\\log(1-a)$ \n","\n","Complete the below function to calculate L(a, y) in python\n","\n","\n","**Hint**: \n","\n","Use np.log() for log\n","\n","Use np.sum for summation \n","\n"]},{"cell_type":"code","metadata":{"id":"77qxCMCci6XZ","colab_type":"code","colab":{}},"source":["def loss(A, Y, m):\n","  \n","  \"\"\"\n","    Returns the average log loss of A and Y\n","\n","    Arguments:\n","    A --> A numpy array of any size.\n","    Y --> A numpy array of any size.\n","    m --> number of samples\n","\n","    Return:\n","    loss(A, Y, m) --> a scalar\n","  \"\"\"\n","  \n","  ## WRITE YOUR CODE HERE ## (~ 1 line of code)\n","  \n","  return \n","\n","  ### END YOUR CODE HERE ###"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"znZddmU9Fepy","colab_type":"text"},"source":["## Initialize the weight (w) and bais (b)\n","\n","**Hint**:\n","\n","use np.zeros() to initialize w \n","\n","Shape of w = (dim, 1)\n","b = 0"]},{"cell_type":"code","metadata":{"id":"DL4vBXzP2nH4","colab_type":"code","colab":{}},"source":["def initialize_weight_bias(dim):\n","  \"\"\"\n","  Initialize the weight (W) and bias (b)\n","  \n","  return w and b\n","  \"\"\"\n","  ## WRITE YOUR CODE HERE ## (~ 2 line of code)\n","  \n","  w = \n","  b = \n","  \n","  ### END YOUR CODE HERE ###\n","  \n","  return w,b"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3CZkBT2XGap1","colab_type":"text"},"source":["## Task-3: Implement Gradient Descent (GD):\n","\n","### Step -1: Forward Pass equations:\n","\n","$A = \\sigma(W^TX + b)$\n","\n","$ cost = L(A, Y)$ --> Loss function implemented above\n","\n","\n","\n","### Step-2: Back Propagation equations:\n","\n","**Calculate $dw$ and $db$**\n","\n","$ dz = \\frac{1}{m} * (A - Y)$\n","\n","$ dw = \\frac{1}{m} X (A - Y)^T $  --> Hint: use $np.dot(X, dz^T)$\n","\n","$ db = \\frac{1}{m} \\sum_{i=1}^m (A - Y)$ --> Hint: use $np.sum(dz)$\n","\n","\n","### Step-3: Update the value of $w$ and $b$:**\n","\n","$w = w - \\alpha dw$\n","\n","$b = b - \\alpha dw$\n","\n"]},{"cell_type":"code","metadata":{"id":"KjYK9cPKTEZ-","colab_type":"code","colab":{}},"source":["def gradient_descent(w, b, X, Y, epochs, learning_rate, show_cost = True):\n","    \"\"\"\n","    Gradient Descent (GD) Algorithm to find the optimum value of W and B\n","    \n","    Arguments:\n","    w --> weights, a numpy array of size (num_px * num_px * 3, 1)\n","    b --> bias, a scalar\n","    X --> data of shape (num_px * num_px * 3, number of examples)\n","    Y --> true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n","    num_iterations --> number of iterations of the GD loop\n","    learning_rate --> learning rate of the gradient descent update rule\n","    show_cost --> True to displat the loss at every 1000 steps\n","    \n","    Returns:\n","    params --> dictionary containing the weights w and bias b\n","    grads --> dictionary containing the gradients of the weights and bias with respect to the cost function\n","    costs --> list of all the costs computed during the optimization, this will be used to plot the learning curve.\n","    \n","    Tips:\n","    You basically need to write down two steps and iterate through them:\n","        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n","        2) Update the parameters using gradient descent rule for w and b.\n","    \"\"\"\n","    \n","    costs = []\n","    \n","    for itr in range(epochs):\n","        \n","        \n","        # FORWARD PASS Operation: Cost and gradient calculation\n","        ## WRITE YOUR CODE HERE ## (~ 2 line of code)\n","        \n","        m = X.shape[1]\n","        \n","        A =       # compute the activation, Hint: call sigmoid()\n","        cost =    # compute cost, Hint: Call the loss()\n","        ### END YOUR CODE HERE ###\n","        \n","        # BACK PROPAGATION Operation (Find gradient(dw), dw and db)\n","        ## WRITE YOUR CODE HERE ## (~ 3 line of code)\n","                \n","        dz= \n","        dw =\n","        db =\n","               \n","        ### END YOUR CODE HERE ###\n","        \n","        assert(dw.shape == w.shape)\n","        \n","        cost = np.squeeze(cost)\n","                       \n","        # Update w and b with dw and db\n","        # Hint:\n","        # dw = dw - learning_rate *dw\n","        # db = db - learning_rate *db\n","        ## WRITE YOUR CODE HERE ## (~ 2 line of code)\n","        \n","        w = \n","        b = \n","        \n","        ### END YOUR CODE HERE ###\n","        #print (\"Cost after %i epochs: %f\" %(itr, cost))\n","        \n","        # Record the costs after 100 interations\n","        if itr % 10 == 0:\n","            costs.append(cost)\n","        \n","        # Print the cost every 100 training examples\n","        if (show_cost and itr % 10 == 0):\n","            print (\"Cost after %i epochs: %f\" %(itr, cost))\n","    \n","    params = {\"w\": w,\n","              \"b\": b}\n","    \n","    grads = {\"dw\": dw,\n","             \"db\": db}\n","    \n","    return params, grads, costs"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o4M6VSEmMM10","colab_type":"text"},"source":["### Implement the function $predict()$:\n"]},{"cell_type":"code","metadata":{"id":"IAzzyw2BVS2N","colab_type":"code","colab":{}},"source":["def predict(wOptimum, bOptimum, XTest, threshold=0.5):\n","    '''\n","    Predict whether the label is 0 or 1 using logistic regression parameters (w, b) learned using Gradient Descent(GD)\n","    \n","    Arguments:\n","    wOptimum --> weights, a numpy array of size (num_px * num_px * 3, 1)\n","    bOptimum --> bias, a scalar\n","    XTest --> data of size (num_px * num_px * 3, number of examples)\n","    \n","    Returns:\n","    Y_prediction --> a numpy array (vector) containing all predictions (0/1) for the examples in X\n","    '''\n","\n","    m = XTest.shape[1]\n","    \n","    Y_prediction = np.zeros((1,m))\n","    wOptimum = wOptimum.reshape(XTest.shape[0], 1)\n","    \n","    ## WRITE YOUR CODE HERE ## (~ 1 line of code)\n","    \n","    A =  # Hint: call sigmoid activation function\n"," \n","    Y_prediction = 1. * (A > threshold)\n","  \n","    ### END YOUR CODE HERE ###\n","    \n","    assert(Y_prediction.shape == (1, m))\n","    \n","    return Y_prediction"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7m-ICy5yMbod","colab_type":"text"},"source":["### Implement the function $fit()$ :\n","\n","Combine all the above functions to create a logistic regression classifier:\n","\n","1. Initialize paramenter $w$ and $b$\n","2. Gradient Descent (Forward Pass --> Back Propagation --> Update $w$, $b$)\n","3. Evaluate the model on training dataset"]},{"cell_type":"code","metadata":{"id":"b65n84EhVnMn","colab_type":"code","colab":{}},"source":["def fit(X_train, Y_train, epochs = 2000, learning_rate = 0.5, show_cost = False):\n","    \"\"\"\n","    Builds the logistic regression model by calling the functions we have implemented previously\n","    \n","    Arguments:\n","    X_train --> training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n","    Y_train --> training labels represented by a numpy array (vector) of shape (1, m_train)\n","    num_iterations --> hyperparameter representing the number of iterations to optimize the parameters\n","    learning_rate --> hyperparameter representing the learning rate used in the update rule of optimize()\n","    show_cost --> Set to true to display the cost every 100 iterations\n","    \n","    Returns:\n","    model --> dictionary containing information (costs, Y_prediction_train, w, b, learning_rate,num_iteration) about the model.\n","    \"\"\"\n","    \n","    ### START CODE HERE ###\n","    \n","    # initialize parameters with zeros (~ 1 line of code)\n","    w, b = \n","\n","    # Gradient descent (~ 1 line of code)\n","    parameters, grads, costs = \n","    \n","    # Retrieve parameters w and b from dictionary \"parameters\"\n","    w = parameters[\"w\"]\n","    b = parameters[\"b\"]\n","    \n","    # Predict train set samples (~ 1 lines of code)\n","    Y_prediction_train = \n","\n","    ### END CODE HERE ###\n","\n","    # Print train/test Errors\n","    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n","    \n","  \n","    model = {\"costs\": costs,\n","         \"Y_prediction_train\" : Y_prediction_train, \n","         \"w\" : w, \n","         \"b\" : b,\n","         \"learning_rate\" : learning_rate,\n","         \"num_iterations\": epochs}\n","    \n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_1mxCldQol3b","colab_type":"code","colab":{}},"source":["def plotLearningCurve(model):\n","  \"\"\"\n","  Helper function to plot the Learning curve\n","  \n","  Argument : model (learned paramenters and costs)\n","  \"\"\"\n","  \n","  plt.plot(model[\"costs\"])\n","  plt.ylabel('cost')\n","  plt.xlabel('iterations (per hundreds)')\n","  plt.title(\"Cost/Loss Curve\")\n","  plt.show()\n","  "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dSZZuSdDtJWl","colab_type":"text"},"source":["###  Mount the Google Drive to access the Cats and Dogs Dataset\n","Reference: https://github.com/ardamavi/Dog-Cat-Classifier\n","\n"]},{"cell_type":"code","metadata":{"id":"wkhugCRrJUgB","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lY65sm5_JuPh","colab_type":"code","colab":{}},"source":["cd /content/gdrive/My Drive/42028-DL-CNN-2020/Week4-Lab4/Cats-Dogs-dataset-32/"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZBmuIeALJ-H5","colab_type":"code","colab":{}},"source":["!ls"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yePSv8k0Ng5D","colab_type":"text"},"source":["### Helper function to read the Cats and Dogs dataset and return the train and test splits (Don't modify)"]},{"cell_type":"code","metadata":{"id":"Ry-kriKKKCKB","colab_type":"code","colab":{}},"source":["def loadDataset():\n","\n","  ## Read all the train and test images and flatten them for training and testing\n","  train_path   = \"./TrainData\"\n","  test_path    = \"./TestData\"\n","  train_labels = os.listdir(train_path)\n","  test_labels  = os.listdir(test_path) \n","\n","  image_size       = (64, 64)\n","  num_train_images = 200\n","  num_test_images  = 100\n","  num_channels     = 3\n","\n","  train_x = np.zeros(((image_size[0]*image_size[1]*num_channels), num_train_images))\n","  train_y = np.zeros((1, num_train_images))\n","  test_x  = np.zeros(((image_size[0]*image_size[1]*num_channels), num_test_images))\n","  test_y  = np.zeros((1, num_test_images))\n","\n","  #----------------\n","  # TRAIN dataset\n","  #----------------\n","  count = 0\n","  num_label = 0\n","  for i, label in enumerate(train_labels):\n","    cur_path = train_path + \"/\" + label\n","    #print(glob.glob(cur_path + \"/*.jpg\"))\n","    for image_path in glob.glob(cur_path + \"/*.jpg\"):\n","      img = image.load_img(image_path, target_size=image_size)\n","      #print(image_path)\n","      x   = image.img_to_array(img)\n","      x   = x.flatten()\n","      x   = np.expand_dims(x, axis=0)\n","      train_x[:,count] = x\n","      train_y[:,count] = num_label\n","      count += 1\n","      #Read only 100 samples for each class for training \n","      if (count==99 or count==199):\n","        break\n","    num_label += 1\n","\n","  #--------------\n","  # TEST dataset\n","  #--------------\n","  count = 0 \n","  num_label = 0 \n","  for i, label in enumerate(test_labels):\n","    cur_path = test_path + \"/\" + label\n","    for image_path in glob.glob(cur_path + \"/*.jpg\"):\n","      img = image.load_img(image_path, target_size=image_size)\n","      x   = image.img_to_array(img)\n","      x   = x.flatten()\n","      x   = np.expand_dims(x, axis=0)\n","      test_x[:,count] = x\n","      test_y[:,count] = num_label\n","      count += 1\n","    num_label += 1\n","\n","  #------------------\n","  # standardization\n","  #------------------\n","  train_x = train_x/255.\n","  test_x  = test_x/255.\n","\n","\n","  ## Print the statistics of the data\n","  print (\"train_labels : \" + str(train_labels))\n","  print (\"train_x shape: \" + str(train_x.shape))\n","  print (\"train_y shape: \" + str(train_y.shape))\n","  print (\"test_x shape : \" + str(test_x.shape))\n","  print (\"test_y shape : \" + str(test_y.shape))\n","\n","  #-----------------\n","  # save using h5py\n","  #-----------------\n","  h5_train = h5py.File(\"train_x.h5\", 'w')\n","  h5_train.create_dataset(\"data_train\", data=np.array(train_x))\n","  h5_train.close()\n","\n","  h5_test = h5py.File(\"test_x.h5\", 'w')\n","  h5_test.create_dataset(\"data_test\", data=np.array(test_x))\n","  h5_test.close()\n","\n","  return train_x, train_y, test_x, test_y\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eMDKRiYeOWLP","colab_type":"text"},"source":["## Task-4: Create a Logistic Regression model and train it"]},{"cell_type":"code","metadata":{"id":"5LvmvXrOV6Ue","colab_type":"code","colab":{}},"source":["\"\"\" Iris dataset\n","iris = sklearn.datasets.load_iris()\n","train_set_x = iris.data[:, :2]\n","train_set_y = (iris.target != 0) * 1\n","test_set_x = train_set_x\n","test_set_y = train_set_y\n","\"\"\"\n","\n","## Read the Cats and Dogs dataset\n","train_set_x, train_set_y, test_set_x, test_set_y = loadDataset()\n","\n","%time \n","## Train a Logistic Regression classifier.\n","model = fit(train_set_x, train_set_y, epochs = 4000, learning_rate = 0.001, show_cost = True)\n","#print(model)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"io8tirw3O9W0","colab_type":"text"},"source":["### Evaluate the Trained model on test dataset using the learned $w$ and $b$"]},{"cell_type":"code","metadata":{"id":"0QnjY42yiZRP","colab_type":"code","colab":{}},"source":["## Retrieve w and b from the trained model for prediction\n","w = model[\"w\"]\n","b = model[\"b\"]\n","\n","%time\n","## Call predict() to test the samples in testset\n","Y_prediction_test = predict(w,b,test_set_x, 0.6)\n","print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - test_set_y)) * 100))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FzC0MRrUPVgy","colab_type":"text"},"source":["### Plot the learning curve"]},{"cell_type":"code","metadata":{"id":"xKSBxophzqZf","colab_type":"code","colab":{}},"source":["plotLearningCurve(model)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EBQq_GsJPoMa","colab_type":"text"},"source":["## Optional: Try to write the function for minibatch_GD with the following function signature:\n","\n","$minibatchGD(X, y, theta, learning-rate=0.1, iter = 10, batch-size=20)$"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"t-h4bTzNPmkG","colab":{}},"source":["## OPTIONAL\n","\n","\n","## Batch Gradient Descent (BGD)\n","\n","def minibatch_GD(X, y, theta, learning_rate=0.1, iter = 10, batch_size=20):\n","  \n","  ### WRITE YOUR CODE HERE ###\n","  \n","  \n","  ### END YOUR CODE ##\n","    \n","  return theta, cost_history\n","\n","      \n","      "],"execution_count":0,"outputs":[]}]}